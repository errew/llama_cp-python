# llama_cp-ppython
自己编译备份


去年使用Comfyui的插件，遇到llama-cpp无法加载.gguf模型，当时放弃使用该插件。
现在又遇到一个关于Qwen-3的量化模型的插件，又是类似的加载问题。
找不到适配的llama-cpp文件的版本，不得以自己编译一个。

我的电脑是4080s、CUDA13.0、python3.12.10所以不保证这个文件适用所有的接近的环境。




其核心参数是 ARCHS = 890，未设置复杂的CMAKE_ARGS，可兼容NVIDIA 40系列显卡。
30系列显卡无法使用。
无法调用50系显卡特有的新特性，50系列要达到最优化需要重新编译。


纯当娱乐。



